# 부스팅 (boosting)
> 부스팅(boosting)은 성능이 약한 학습기(weak learner)를 여러 개 연결하여 강한 학습기(strong learner)를 만드는 앙상블 학습이다.
> 부스팅 방법의 아이디어는 앞에서 학습된 모델을 보완해나가면서 더나은 모델로 학습시키는 것이다. 부스팅 방법에는 여러 종류가 있는데,
> 그 중 가장 유명하고 인기 있는 모델은 아다부스트(AdaBoost, Adaptive Boosting)와 그래디언트 부스팅(Gradient Boosting) 두 가지가 있다.

## 아다부스트 (AdaBoost)
> 아다부스트(AdaBoost)는 과소적합(underfitted)됐던 학습 데이터 샘플의 가중치를 높이면서 새로 학습된 모델이 학습하기 어려운 데이터에 더 잘 적합되도록 하는 방식이다. 아래의 그림에서 처럼 아다부스트의 단계를 설명하면 다음과 같다.

- 먼저 전제 학습 데이터셋을 이용해 모델을 만든 후, 잘못 예측(분류)된 샘플의 가중치를 상대적으로 높여준다.
- 그 다음 두 번째 모델을 학습 시킬때 이렇게 업데이트된 가중치를 반영하여 모델을 학습 시킨다.
- 이와 같은 과정을 반복한다.

![1](https://github.com/jaeb0129/baseball/assets/63768509/ab654ded-a217-45d6-b28d-3397c935cead)

## 그래디언트 부스팅(Gradient Boosting)
> 그래디언트 부스팅은 '아다부스트'에서 살펴본 것 처럼 전의 학습된 모델의 오차를 보완하는 방향으로 모델(분류기, 학습기)을 추가해주는 방법은 동일하다.
> 하지만, 그래디언트 부스팅은 아다부스트 처럼 학습단계 마다 데이터 샘플의 가중치를 업데이트 해주는 것이 아니라 학습 전단계 모델에서의 잔여 오차(residual error)에 대해 새로운 모델을 학습시키는 방법이다.

![2](https://github.com/jaeb0129/baseball/assets/63768509/8f56fd1f-5e5f-44b9-ba77-827cada79413)

### 문제점
- 느리다
- 과적합(overfitting)의 이슈가 있다

## xgBOOST

- 이런 앙상블 부스팅 모델에서 더 성능이 좋게 만들어진 여러 모델 나옴
- xgBoost 모델도 그렇게 탄생
- gradient boosting 알고리즘의 단점 보완해주기 위해 나옴

> 과적합 방지 가능한 규제가 포함

> 분류, 회귀가 둘 다 가능

> 조기 종료(Early Stopping) 제공

> 결국 gradient Boost 기반
